{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13704598,"sourceType":"datasetVersion","datasetId":8717947},{"sourceId":13705491,"sourceType":"datasetVersion","datasetId":8718562}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T17:31:31.090968Z","iopub.execute_input":"2025-11-13T17:31:31.091542Z","iopub.status.idle":"2025-11-13T17:31:31.358874Z","shell.execute_reply.started":"2025-11-13T17:31:31.091511Z","shell.execute_reply":"2025-11-13T17:31:31.358229Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/post-images/Hanumannagar_Postflood_Orthomosaic.tif\n/kaggle/input/pre-images/Hanumannagar_Preflood_Orthomosaic.tif\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch \nimport torch.nn as nn \nimport torch.optim as optim \nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom torch.utils.data import Dataset, DataLoader \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T01:14:23.235369Z","iopub.execute_input":"2025-11-14T01:14:23.236123Z","iopub.status.idle":"2025-11-14T01:14:23.783752Z","shell.execute_reply.started":"2025-11-14T01:14:23.236096Z","shell.execute_reply":"2025-11-14T01:14:23.783170Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"!pip install rasterio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T00:52:30.912950Z","iopub.execute_input":"2025-11-14T00:52:30.913503Z","iopub.status.idle":"2025-11-14T00:52:36.377212Z","shell.execute_reply.started":"2025-11-14T00:52:30.913478Z","shell.execute_reply":"2025-11-14T00:52:36.376524Z"}},"outputs":[{"name":"stdout","text":"Collecting rasterio\n  Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\nCollecting affine (from rasterio)\n  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\nRequirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio) (25.4.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from rasterio) (2025.10.5)\nRequirement already satisfied: click>=4.0 in /usr/local/lib/python3.11/dist-packages (from rasterio) (8.3.0)\nRequirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.11/dist-packages (from rasterio) (0.7.2)\nRequirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from rasterio) (1.26.4)\nRequirement already satisfied: click-plugins in /usr/local/lib/python3.11/dist-packages (from rasterio) (1.1.1.2)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from rasterio) (3.0.9)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->rasterio) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->rasterio) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->rasterio) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->rasterio) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->rasterio) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->rasterio) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24->rasterio) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24->rasterio) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24->rasterio) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.24->rasterio) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.24->rasterio) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.24->rasterio) (2024.2.0)\nDownloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading affine-2.4.0-py3-none-any.whl (15 kB)\nInstalling collected packages: affine, rasterio\nSuccessfully installed affine-2.4.0 rasterio-1.4.3\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Explore","metadata":{}},{"cell_type":"code","source":"preflood_tiles = []\n\nwith rasterio.open(\"/kaggle/input/pre-images/Hanumannagar_Preflood_Orthomosaic.tif\") as src:\n    for y in range(0, src.height, tile_size):\n        for x in range(0, src.width, tile_size):\n            window = Window(x, y, tile_size, tile_size)\n            img = src.read(window=window)  \n            preflood_tiles.append(img)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Function to normalize each tile safely\ndef normalize_tile(tile):\n    tile = tile.astype(float)\n    min_val, max_val = np.nanmin(tile), np.nanmax(tile)\n    if max_val > min_val:\n        tile = (tile - min_val) / (max_val - min_val)\n    else:\n        tile = np.zeros_like(tile)\n    return tile\n\n# Parameters\nstart_idx = 200       # first tile index\nend_idx = 400       # last tile index\ntiles_to_show = tiles[start_idx:end_idx]\nn_tiles = len(tiles_to_show)\n\ncols = 10             # number of columns in grid\nrows = n_tiles // cols + (n_tiles % cols > 0)\n\nfig, axes = plt.subplots(rows, cols, figsize=(20, 2*rows))\naxes = axes.flatten()  # flatten to 1D for easy indexing\n\nfor i, tile in enumerate(tiles_to_show):\n    rgb = tile[:3, :, :]                      # take R,G,B\n    rgb = np.transpose(rgb, (1, 2, 0))       # (C,H,W) -> (H,W,C)\n    rgb = normalize_tile(rgb)\n    \n    axes[i].imshow(rgb)\n    axes[i].axis('off')\n    axes[i].set_title(f\"Tile {start_idx+i}\", fontsize=8)\n\n# Hide any empty subplots\nfor j in range(i+1, len(axes)):\n    axes[j].axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T18:25:00.454932Z","iopub.execute_input":"2025-11-13T18:25:00.455314Z","iopub.status.idle":"2025-11-13T18:25:00.465925Z","shell.execute_reply.started":"2025-11-13T18:25:00.455284Z","shell.execute_reply":"2025-11-13T18:25:00.465099Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7c6b92db9a10>"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\nprint(\"Device: \", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T18:25:00.698126Z","iopub.execute_input":"2025-11-13T18:25:00.698399Z","iopub.status.idle":"2025-11-13T18:25:00.773656Z","shell.execute_reply.started":"2025-11-13T18:25:00.698378Z","shell.execute_reply":"2025-11-13T18:25:00.772754Z"}},"outputs":[{"name":"stdout","text":"Device:  cuda\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# pesudo mask generation (try)","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport rasterio\nfrom rasterio.windows import Window, from_bounds\nfrom rasterio.coords import disjoint_bounds\nimport cv2\nfrom PIL import Image\n\n# ---- NDWI calculation ----\ndef compute_ndwi(tile):\n    green = tile[..., 1].astype('float32')\n    nir = tile[..., 3].astype('float32')\n    ndwi = (green - nir) / (green + nir + 1e-8)\n    return ndwi\n\n# ---- if RGB only ----\ndef compute_rgb_diff(pre, post):\n    gray_pre = cv2.cvtColor(pre[..., :3], cv2.COLOR_RGB2GRAY).astype('float32')\n    gray_post = cv2.cvtColor(post[..., :3], cv2.COLOR_RGB2GRAY).astype('float32')\n    diff = cv2.absdiff(gray_post, gray_pre)\n    diff = cv2.GaussianBlur(diff, (5,5), 0)\n    diff_norm = (diff - diff.min()) / (diff.max() - diff.min() + 1e-8)\n    return diff_norm\n\n# ---- threshold function ----\ndef binarize(image, method='otsu', thresh=None):\n    img8 = np.uint8(image * 255)\n    if method == 'otsu':\n        _, mask = cv2.threshold(img8, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    else:\n        _, mask = cv2.threshold(img8, thresh, 255, cv2.THRESH_BINARY)\n    return mask // 255\n\n# ---- align and crop to overlap ----\ndef align_and_crop(pre_src, post_src):\n    if pre_src.crs != post_src.crs:\n        raise ValueError(\" CRS mismatch! Please reproject before alignment.\")\n\n    if disjoint_bounds(pre_src.bounds, post_src.bounds):\n        raise ValueError(\" No overlapping region between pre and post rasters.\")\n\n    # Compute overlap bounds\n    left = max(pre_src.bounds.left, post_src.bounds.left)\n    right = min(pre_src.bounds.right, post_src.bounds.right)\n    bottom = max(pre_src.bounds.bottom, post_src.bounds.bottom)\n    top = min(pre_src.bounds.top, post_src.bounds.top)\n\n    # Create windows for each\n    win_pre = from_bounds(left, bottom, right, top, pre_src.transform)\n    win_post = from_bounds(left, bottom, right, top, post_src.transform)\n\n    # Read overlapping data\n    pre_data = pre_src.read(window=win_pre)\n    post_data = post_src.read(window=win_post)\n\n    # Convert to HWC\n    pre_data = np.transpose(pre_data, (1,2,0))\n    post_data = np.transpose(post_data, (1,2,0))\n\n    print(f\"‚úÖ Cropped to overlap area: {pre_data.shape}\")\n    return pre_data, post_data\n\n# ---- main function ----\ndef generate_pseudo_masks(pre_tif, post_tif, out_dir, tile_size=512, min_area_px=50):\n    os.makedirs(out_dir, exist_ok=True)\n    with rasterio.open(pre_tif) as pre_src, rasterio.open(post_tif) as post_src:\n        pre, post = align_and_crop(pre_src, post_src)\n\n        h, w = pre.shape[:2]\n        print(\"Aligned image shape:\", h, w)\n        tid = 0\n\n        for y in range(0, h, tile_size):\n            for x in range(0, w, tile_size):\n                wsize = min(tile_size, w - x)\n                hsize = min(tile_size, h - y)\n                pre_tile = pre[y:y+hsize, x:x+wsize]\n                post_tile = post[y:y+hsize, x:x+wsize]\n\n                # ----- Compute change mask -----\n                if pre_tile.shape[-1] >= 4:  # NIR available\n                    ndwi_pre = compute_ndwi(pre_tile)\n                    ndwi_post = compute_ndwi(post_tile)\n                    water_pre = binarize((ndwi_pre - ndwi_pre.min())/(ndwi_pre.max()-ndwi_pre.min()))\n                    water_post = binarize((ndwi_post - ndwi_post.min())/(ndwi_post.max()-ndwi_post.min()))\n                    mask = (water_post == 1) & (water_pre == 0)\n                else:\n                    diff = compute_rgb_diff(pre_tile, post_tile)\n                    mask = binarize(diff, 'otsu')\n\n                mask = mask.astype('uint8')\n                mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((3,3),np.uint8))\n                if np.sum(mask) < min_area_px:\n                    continue\n\n                # ---- Save tile ----\n                pre_rgb = pre_tile[..., :3]\n                post_rgb = post_tile[..., :3]\n                pre_png = os.path.join(out_dir, f\"tile_{tid}_pre.png\")\n                post_png = os.path.join(out_dir, f\"tile_{tid}_post.png\")\n                mask_png = os.path.join(out_dir, f\"tile_{tid}_mask.png\")\n\n                def save_png(img, path):\n                    img = np.clip(img, 0, np.percentile(img, 99))\n                    img = ((img - img.min()) / (img.max() - img.min() + 1e-8) * 255).astype(np.uint8)\n                    Image.fromarray(img).save(path)\n\n                save_png(pre_rgb, pre_png)\n                save_png(post_rgb, post_png)\n                Image.fromarray(mask*255).save(mask_png)\n\n                tid += 1\n\n        print(f\"‚úÖ Saved {tid} aligned tiles with pseudo masks in {out_dir}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generate_pseudo_masks(\n    \"/kaggle/input/pre-images/Hanumannagar_Preflood_Orthomosaic.tif\",\n    \"/kaggle/input/post-images/Hanumannagar_Postflood_Orthomosaic.tif\",\n    \"/kaggle/working/masks\"\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-13T13:33:29.950Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import rasterio\nwith rasterio.open(\"/kaggle/input/pre-images/Hanumannagar_Preflood_Orthomosaic.tif\") as pre_src, rasterio.open(\"/kaggle/input/post-images/Hanumannagar_Postflood_Orthomosaic.tif\") as post_src:\n    print(pre_src.width, pre_src.height)\n    print(pre_src.crs)\n    print(post_src.width, post_src.height)\n    print(post_src.crs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Align and Crop ","metadata":{}},{"cell_type":"code","source":"import os\nimport rasterio\nfrom rasterio.windows import from_bounds, Window\nimport math\n\ndef align_and_crop_to_overlap(pre_path, post_path, out_pre, out_post, block_size=1024):\n    # Ensure output directories exist\n    os.makedirs(os.path.dirname(out_pre), exist_ok=True)\n    os.makedirs(os.path.dirname(out_post), exist_ok=True)\n\n    with rasterio.open(pre_path) as pre, rasterio.open(post_path) as post:\n        # Step 1: CRS check\n        if pre.crs != post.crs:\n            raise ValueError(\"‚ùå CRS mismatch! Please reproject first.\")\n\n        # Step 2: Compute overlap\n        left = max(pre.bounds.left, post.bounds.left)\n        right = min(pre.bounds.right, post.bounds.right)\n        bottom = max(pre.bounds.bottom, post.bounds.bottom)\n        top = min(pre.bounds.top, post.bounds.top)\n        print(f\"‚úÖ Overlap bounds: {left:.2f}, {bottom:.2f}, {right:.2f}, {top:.2f}\")\n\n        # Step 3: Create read windows\n        win_pre = from_bounds(left, bottom, right, top, pre.transform)\n        win_post = from_bounds(left, bottom, right, top, post.transform)\n\n        # Step 4: Output transform and shape\n        transform = pre.window_transform(win_pre)\n        height = int(win_pre.height)\n        width = int(win_pre.width)\n        print(f\"‚úÖ Overlap size: {width} x {height}\")\n\n        # Step 5: Update output profile\n        profile = pre.profile\n        profile.update({\n            'height': height,\n            'width': width,\n            'transform': transform,\n            'BIGTIFF': 'YES'  # ‚úÖ Enable BigTIFF output\n        })\n\n        # Step 6: Write tile-by-tile\n        with rasterio.open(out_pre, 'w', **profile) as dst_pre, \\\n             rasterio.open(out_post, 'w', **profile) as dst_post:\n\n            num_tiles_x = math.ceil(width / block_size)\n            num_tiles_y = math.ceil(height / block_size)\n            print(f\"üß© Processing in {num_tiles_x} x {num_tiles_y} tiles...\")\n\n            for ty in range(num_tiles_y):\n                for tx in range(num_tiles_x):\n                    x_off = int(tx * block_size)\n                    y_off = int(ty * block_size)\n                    w = min(block_size, width - x_off)\n                    h = min(block_size, height - y_off)\n\n                    window_pre = Window(win_pre.col_off + x_off, win_pre.row_off + y_off, w, h)\n                    window_post = Window(win_post.col_off + x_off, win_post.row_off + y_off, w, h)\n\n                    pre_block = pre.read(window=window_pre)\n                    post_block = post.read(window=window_post)\n\n                    dst_pre.write(pre_block, window=Window(x_off, y_off, w, h))\n                    dst_post.write(post_block, window=Window(x_off, y_off, w, h))\n\n            print(f\"‚úÖ Finished writing aligned cropped TIFFs:\\n - {out_pre}\\n - {out_post}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T00:52:50.096700Z","iopub.execute_input":"2025-11-14T00:52:50.097558Z","iopub.status.idle":"2025-11-14T00:52:50.743449Z","shell.execute_reply.started":"2025-11-14T00:52:50.097521Z","shell.execute_reply":"2025-11-14T00:52:50.742858Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"align_and_crop_to_overlap(\n    \"/kaggle/input/pre-images/Hanumannagar_Preflood_Orthomosaic.tif\",\n    \"/kaggle/input/post-images/Hanumannagar_Postflood_Orthomosaic.tif\",\n    \"/kaggle/working/aligned/pre_aligned.tif\",\n    \"/kaggle/working/aligned/post_aligned.tif\",\n    block_size=1024  # tune for memory vs speed\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T00:52:51.153474Z","iopub.execute_input":"2025-11-14T00:52:51.154283Z","iopub.status.idle":"2025-11-14T01:10:36.789438Z","shell.execute_reply.started":"2025-11-14T00:52:51.154260Z","shell.execute_reply":"2025-11-14T01:10:36.788676Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Overlap bounds: 489425.93, 2935667.56, 491944.07, 2939394.83\n‚úÖ Overlap size: 58395 x 86434\nüß© Processing in 58 x 85 tiles...\n‚úÖ Finished writing aligned cropped TIFFs:\n - /kaggle/working/aligned/pre_aligned.tif\n - /kaggle/working/aligned/post_aligned.tif\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport rasterio\nimport numpy as np\nimport cv2\n\nclass FloodDataset(Dataset):\n    def __init__(self, pre_path, post_path, tile_size=512, augment=None, use_ndwi=True):\n        self.pre_path = pre_path\n        self.post_path = post_path\n        self.tile_size = tile_size\n        self.augment = augment\n        self.use_ndwi = use_ndwi\n\n        with rasterio.open(pre_path) as src:\n            self.width = src.width\n            self.height = src.height\n            self.geo_transform = src.transform\n            self.crs = src.crs\n\n        self.tiles_x = (self.width + tile_size - 1) // tile_size\n        self.tiles_y = (self.height + tile_size - 1) // tile_size\n        self.total_tiles = self.tiles_x * self.tiles_y\n\n    def __len__(self):\n        return self.total_tiles\n\n    def _compute_ndwi(self, tile):\n        green = tile[..., 1].astype('float32')\n        nir = tile[..., 3].astype('float32')\n        ndwi = (green - nir) / (green + nir + 1e-8)\n        return ndwi\n\n    def _compute_rgb_diff(self, pre, post):\n        gray_pre = cv2.cvtColor(pre[..., :3], cv2.COLOR_RGB2GRAY).astype('float32')\n        gray_post = cv2.cvtColor(post[..., :3], cv2.COLOR_RGB2GRAY).astype('float32')\n        diff = cv2.absdiff(gray_post, gray_pre)\n        diff = cv2.GaussianBlur(diff, (5, 5), 0)\n        diff_norm = (diff - diff.min()) / (diff.max() - diff.min() + 1e-8)\n        return diff_norm\n\n    def _binarize(self, image):\n        img8 = np.uint8(image * 255)\n        _, mask = cv2.threshold(img8, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n        return mask // 255\n\n    def __getitem__(self, idx):\n        x_idx = idx % self.tiles_x\n        y_idx = idx // self.tiles_x\n        x = x_idx * self.tile_size\n        y = y_idx * self.tile_size\n        window = rasterio.windows.Window(x, y, self.tile_size, self.tile_size)\n    \n        with rasterio.open(self.pre_path) as pre_src, rasterio.open(self.post_path) as post_src:\n            pre_tile = np.transpose(pre_src.read(window=window), (1,2,0))\n            post_tile = np.transpose(post_src.read(window=window), (1,2,0))\n    \n        # --- pad to tile_size if needed ---\n        def pad_to_tile(tile):\n            h, w, c = tile.shape\n            pad_h = max(0, self.tile_size - h)\n            pad_w = max(0, self.tile_size - w)\n            if pad_h > 0 or pad_w > 0:\n                tile = np.pad(tile, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"reflect\")\n            return tile[:self.tile_size, :self.tile_size, :]\n    \n        pre_tile = pad_to_tile(pre_tile)\n        post_tile = pad_to_tile(post_tile)\n    \n        # Normalize\n        pre_tile = pre_tile.astype(\"float32\")\n        post_tile = post_tile.astype(\"float32\")\n        pre_tile /= pre_tile.max() if pre_tile.max() > 1 else 1\n        post_tile /= post_tile.max() if post_tile.max() > 1 else 1\n    \n        # Compute flood mask\n        if self.use_ndwi and pre_tile.shape[-1] >= 4:\n            ndwi_pre = self._compute_ndwi(pre_tile)\n            ndwi_post = self._compute_ndwi(post_tile)\n            mask = (self._binarize(ndwi_post) == 1) & (self._binarize(ndwi_pre) == 0)\n        else:\n            diff = self._compute_rgb_diff(pre_tile, post_tile)\n            mask = self._binarize(diff)\n    \n        mask = mask.astype(\"float32\")[None, :, :]\n    \n        pre_tile = torch.from_numpy(np.transpose(pre_tile[..., :3], (2, 0, 1)))\n        post_tile = torch.from_numpy(np.transpose(post_tile[..., :3], (2, 0, 1)))\n        mask = torch.from_numpy(mask)\n    \n        if self.augment:\n            pre_tile, post_tile, mask = self.augment(pre_tile, post_tile, mask)\n    \n        center_x = x + self.tile_size // 2\n        center_y = y + self.tile_size // 2\n    \n        return pre_tile, post_tile, mask, (center_x, center_y), (x_idx, y_idx)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T01:13:34.313271Z","iopub.execute_input":"2025-11-14T01:13:34.313869Z","iopub.status.idle":"2025-11-14T01:13:38.040660Z","shell.execute_reply.started":"2025-11-14T01:13:34.313841Z","shell.execute_reply":"2025-11-14T01:13:38.039858Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"dataset = FloodDataset(\n    \"/kaggle/working/aligned/pre_aligned.tif\",\n    \"/kaggle/working/aligned/post_aligned.tif\",\n    512\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T01:13:48.051392Z","iopub.execute_input":"2025-11-14T01:13:48.052064Z","iopub.status.idle":"2025-11-14T01:13:48.061934Z","shell.execute_reply.started":"2025-11-14T01:13:48.052040Z","shell.execute_reply":"2025-11-14T01:13:48.061234Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"len(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T01:13:51.388723Z","iopub.execute_input":"2025-11-14T01:13:51.389340Z","iopub.status.idle":"2025-11-14T01:13:51.394500Z","shell.execute_reply.started":"2025-11-14T01:13:51.389312Z","shell.execute_reply":"2025-11-14T01:13:51.393918Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"19435"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"## Train and Val Split","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Subset\n\ndef split_flood_dataset(dataset, val_fraction=0.2):\n    \n    tiles_x, tiles_y = dataset.tiles_x, dataset.tiles_y\n    \n    # Number of rows for validation\n    val_rows = int(tiles_y * val_fraction)\n    \n    # Assign tiles: last val_rows for validation, remaining for training\n    train_indices = []\n    val_indices = []\n    \n    for y in range(tiles_y):\n        for x in range(tiles_x):\n            idx = y * tiles_x + x\n            if y >= tiles_y - val_rows:\n                val_indices.append(idx)\n            else:\n                train_indices.append(idx)\n    \n    return train_indices, val_indices\n\ntrain_idx, val_idx = split_flood_dataset(dataset, val_fraction=0.2)\n\n# split\ntrain_dataset = Subset(dataset, train_idx)\nval_dataset = Subset(dataset, val_idx)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T01:13:56.637732Z","iopub.execute_input":"2025-11-14T01:13:56.638693Z","iopub.status.idle":"2025-11-14T01:13:56.646354Z","shell.execute_reply.started":"2025-11-14T01:13:56.638655Z","shell.execute_reply":"2025-11-14T01:13:56.645634Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# size\nprint(\"size:\\ntrain: \",len(train_dataset),\", val:\",len(val_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T01:14:03.029258Z","iopub.execute_input":"2025-11-14T01:14:03.029981Z","iopub.status.idle":"2025-11-14T01:14:03.033494Z","shell.execute_reply.started":"2025-11-14T01:14:03.029959Z","shell.execute_reply":"2025-11-14T01:14:03.032703Z"}},"outputs":[{"name":"stdout","text":"size:\ntrain:  15640 , val: 3795\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# dataloader \ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2, pin_memory=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T01:14:29.537819Z","iopub.execute_input":"2025-11-14T01:14:29.538533Z","iopub.status.idle":"2025-11-14T01:14:29.542872Z","shell.execute_reply.started":"2025-11-14T01:14:29.538508Z","shell.execute_reply":"2025-11-14T01:14:29.542036Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Siamese U-Net++ Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass DoubleConv(nn.Module):\n\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass SiameseUNetPP(nn.Module):\n   \n    def __init__(self, n_channels=3, n_classes=1, base_ch=64):\n        super().__init__()\n        self.base_ch = base_ch\n\n        # ---------- Encoder (shared weights) ----------\n        self.enc_conv0 = DoubleConv(n_channels, base_ch)\n        self.enc_conv1 = DoubleConv(base_ch, base_ch * 2)\n        self.enc_conv2 = DoubleConv(base_ch * 2, base_ch * 4)\n        self.enc_conv3 = DoubleConv(base_ch * 4, base_ch * 8)\n        self.enc_conv4 = DoubleConv(base_ch * 8, base_ch * 16)\n        self.pool = nn.MaxPool2d(2)\n\n        # ---------- Decoder (nested dense connections) ----------\n        # channels[i] = base_ch * (2 ** i)\n        ch = [base_ch, base_ch*2, base_ch*4, base_ch*8, base_ch*16]\n\n        # For Siamese fusion, encoder outputs are doubled in channels\n        # Define decoder convolutions following full U-Net++ dense pattern\n        self.dconv3_0 = DoubleConv(ch[3]*2 + ch[4]*2, ch[3])               # X_3,0\n        self.dconv2_0 = DoubleConv(ch[2]*2 + ch[3], ch[2])                 # X_2,0\n        self.dconv2_1 = DoubleConv(ch[2] + ch[3], ch[2])                   # X_2,1\n        self.dconv1_0 = DoubleConv(ch[1]*2 + ch[2], ch[1])                 # X_1,0\n        self.dconv1_1 = DoubleConv(ch[1] + ch[2], ch[1])                   # X_1,1\n        self.dconv1_2 = DoubleConv(ch[1] + ch[2], ch[1])                   # X_1,2\n        self.dconv0_0 = DoubleConv(ch[0]*2 + ch[1], ch[0])                 # X_0,0\n        self.dconv0_1 = DoubleConv(ch[0] + ch[1], ch[0])                   # X_0,1\n        self.dconv0_2 = DoubleConv(ch[0] + ch[1], ch[0])                   # X_0,2\n        self.dconv0_3 = DoubleConv(ch[0] + ch[1], ch[0])                   # X_0,3\n\n        # ---------- Output layers (deep supervision) ----------\n        self.out_convs = nn.ModuleList([\n            nn.Conv2d(ch[0], n_classes, kernel_size=1),\n            nn.Conv2d(ch[0], n_classes, kernel_size=1),\n            nn.Conv2d(ch[0], n_classes, kernel_size=1),\n            nn.Conv2d(ch[0], n_classes, kernel_size=1)\n        ])\n\n    # Utility upsampling\n    def _upsample(self, x, ref):\n        \"\"\"Upsample x to match spatial size of ref using bilinear interpolation\"\"\"\n        return F.interpolate(x, size=ref.shape[2:], mode='bilinear', align_corners=False)\n\n    def forward(self, x1, x2, return_deep=False):\n        # ---------- Encoder (Siamese shared weights) ----------\n        e0_1 = self.enc_conv0(x1)\n        e1_1 = self.enc_conv1(self.pool(e0_1))\n        e2_1 = self.enc_conv2(self.pool(e1_1))\n        e3_1 = self.enc_conv3(self.pool(e2_1))\n        e4_1 = self.enc_conv4(self.pool(e3_1))\n\n        e0_2 = self.enc_conv0(x2)\n        e1_2 = self.enc_conv1(self.pool(e0_2))\n        e2_2 = self.enc_conv2(self.pool(e1_2))\n        e3_2 = self.enc_conv3(self.pool(e2_2))\n        e4_2 = self.enc_conv4(self.pool(e3_2))\n\n        # Siamese fusion (concat pre & post)\n        e0 = torch.cat([e0_1, e0_2], dim=1)\n        e1 = torch.cat([e1_1, e1_2], dim=1)\n        e2 = torch.cat([e2_1, e2_2], dim=1)\n        e3 = torch.cat([e3_1, e3_2], dim=1)\n        e4 = torch.cat([e4_1, e4_2], dim=1)\n\n        # ---------- Decoder (full dense path) ----------\n        # Level 3\n        X_3_0 = self.dconv3_0(torch.cat([e3, self._upsample(e4, e3)], dim=1))\n\n        # Level 2\n        X_2_0 = self.dconv2_0(torch.cat([e2, self._upsample(X_3_0, e2)], dim=1))\n        X_2_1 = self.dconv2_1(torch.cat([X_2_0, self._upsample(X_3_0, X_2_0)], dim=1))\n\n        # Level 1\n        X_1_0 = self.dconv1_0(torch.cat([e1, self._upsample(X_2_0, e1)], dim=1))\n        X_1_1 = self.dconv1_1(torch.cat([X_1_0, self._upsample(X_2_1, X_1_0)], dim=1))\n        X_1_2 = self.dconv1_2(torch.cat([X_1_1, self._upsample(X_2_1, X_1_1)], dim=1))\n\n        # Level 0\n        X_0_0 = self.dconv0_0(torch.cat([e0, self._upsample(X_1_0, e0)], dim=1))\n        X_0_1 = self.dconv0_1(torch.cat([X_0_0, self._upsample(X_1_1, X_0_0)], dim=1))\n        X_0_2 = self.dconv0_2(torch.cat([X_0_1, self._upsample(X_1_2, X_0_1)], dim=1))\n        X_0_3 = self.dconv0_3(torch.cat([X_0_2, self._upsample(X_1_2, X_0_2)], dim=1))\n\n        # ---------- Deep supervision outputs ----------\n        out0 = self.out_convs[0](X_0_0)\n        out1 = self.out_convs[1](X_0_1)\n        out2 = self.out_convs[2](X_0_2)\n        out3 = self.out_convs[3](X_0_3)\n\n        deep_outs = [out0, out1, out2, out3]\n\n        if return_deep:\n            return deep_outs\n        else:\n            # Average the deep supervision maps\n            return torch.sigmoid(torch.mean(torch.stack(deep_outs), dim=0))\n\n\n# # # ---------- Example usage ----------\n# if __name__ == \"__main__\":\n#     model = SiameseUNetPP(n_channels=3, n_classes=1, base_ch=64)\n#     x1 = torch.randn(1, 3, 256, 256)\n#     x2 = torch.randn(1, 3, 256, 256)\n#     y = model(x1, x2)\n#     print(\"Output shape:\", y.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T01:14:38.574090Z","iopub.execute_input":"2025-11-14T01:14:38.574758Z","iopub.status.idle":"2025-11-14T01:14:38.591214Z","shell.execute_reply.started":"2025-11-14T01:14:38.574732Z","shell.execute_reply":"2025-11-14T01:14:38.590344Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# model visualization \n!pip install torchviz","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchviz import make_dot\nimport torch\n\n# Create two random sample inputs (batch, channel, height, width)\nsample_input1 = torch.rand(1, 3, 512, 512)\nsample_input2 = torch.rand(1, 3, 512, 512)\n\n# Forward pass through the model with both inputs\noutput = model(sample_input1, sample_input2)\n\n# Visualize model graph\ndot = make_dot(output, params=dict(model.named_parameters()))\ndot.format = 'png'\ndot.render('model_architecture')\n\nfrom IPython.display import Image\nImage(\"model_architecture.png\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Trainig ","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset\nfrom tqdm import tqdm\nimport numpy as np\n\n# ---------------------------\n# 1Ô∏è‚É£ Loss Function\n# ---------------------------\nclass DiceBCELoss(nn.Module):\n    def __init__(self, smooth=1.0):\n        super().__init__()\n        self.smooth = smooth\n        self.bce = nn.BCEWithLogitsLoss()\n\n    def forward(self, preds, targets):\n        preds = preds.contiguous()\n        targets = targets.contiguous()\n\n        bce_loss = self.bce(preds, targets)\n\n        preds = torch.sigmoid(preds)\n        intersection = (preds * targets).sum(dim=(2,3))\n        dice = (2. * intersection + self.smooth) / (\n            preds.sum(dim=(2,3)) + targets.sum(dim=(2,3)) + self.smooth\n        )\n        dice_loss = 1 - dice.mean()\n\n        return bce_loss + dice_loss\n\n\n# ---------------------------\n# 2Ô∏è‚É£ Dice Metric\n# ---------------------------\ndef dice_score(preds, targets, threshold=0.5):\n    preds = torch.sigmoid(preds)\n    preds = (preds > threshold).float()\n    intersection = (preds * targets).sum()\n    union = preds.sum() + targets.sum()\n    dice = (2.0 * intersection) / (union + 1e-8)\n    return dice.item()\n\n\n# ---------------------------\n# 3Ô∏è‚É£ Train & Validate Epoch\n# ---------------------------\ndef train_one_epoch(model, dataloader, optimizer, loss_fn, device, scaler=None):\n    model.train()\n    epoch_loss = 0\n\n    loop = tqdm(dataloader, desc=\"Training\", leave=False)\n    for pre_img, post_img, mask, _, _ in loop:  # <-- added mask + coords\n        pre_img, post_img, mask = pre_img.to(device), post_img.to(device), mask.to(device)\n\n        optimizer.zero_grad()\n\n        with torch.amp.autocast('cuda', enabled=scaler is not None):\n            outputs = model(pre_img, post_img)\n            loss = loss_fn(outputs, mask)\n\n        if scaler:\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            loss.backward()\n            optimizer.step()\n\n        epoch_loss += loss.item()\n        loop.set_postfix(loss=loss.item())\n\n    return epoch_loss / len(dataloader)\n\n\ndef validate_one_epoch(model, dataloader, loss_fn, device):\n    model.eval()\n    epoch_loss = 0\n    epoch_dice = 0\n\n    with torch.no_grad():\n        loop = tqdm(dataloader, desc=\"Validating\", leave=False)\n        for pre_img, post_img, mask, _, _ in loop:\n            pre_img, post_img, mask = pre_img.to(device), post_img.to(device), mask.to(device)\n\n            outputs = model(pre_img, post_img)\n            loss = loss_fn(outputs, mask)\n            epoch_loss += loss.item()\n            epoch_dice += dice_score(outputs, mask)\n\n    return epoch_loss / len(dataloader), epoch_dice / len(dataloader)\n\n\n# ---------------------------\n# 4Ô∏è‚É£ Early Stopping\n# ---------------------------\nclass EarlyStopping:\n    def __init__(self, patience=10, delta=0.0, path=\"best_model.pth\"):\n        self.patience = patience\n        self.delta = delta\n        self.path = path\n        self.best_loss = float(\"inf\")\n        self.counter = 0\n\n    def step(self, val_loss, model):\n        if val_loss < self.best_loss - self.delta:\n            self.best_loss = val_loss\n            self.counter = 0\n            torch.save(model.state_dict(), self.path)\n            print(f\"‚úÖ Model improved ‚Äî saved to {self.path}\")\n        else:\n            self.counter += 1\n            print(f\"‚ö†Ô∏è EarlyStopping counter: {self.counter}/{self.patience}\")\n        return self.counter >= self.patience\n\n\n# ---------------------------\n# 5Ô∏è‚É£ Training Loop\n# ---------------------------\ndef train_model(model, train_loader, val_loader, device, epochs=50, lr=1e-4, ckpt_path=\"best_model.pth\"):\n    model = model.to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n    loss_fn = DiceBCELoss()\n    scaler = torch.amp.GradScaler('cuda', enabled=(device.type == \"cuda\"))\n    early_stopper = EarlyStopping(patience=3, path=ckpt_path)\n\n    for epoch in range(epochs):\n        print(f\"\\nüåä Epoch [{epoch+1}/{epochs}]\")\n        train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device, scaler)\n        val_loss, val_dice = validate_one_epoch(model, val_loader, loss_fn, device)\n\n        print(f\"üìä Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Dice: {val_dice:.4f}\")\n\n        if early_stopper.step(val_loss, model):\n            print(\"üõë Early stopping triggered.\")\n            break\n\n    model.load_state_dict(torch.load(ckpt_path))\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T01:18:37.694461Z","iopub.execute_input":"2025-11-14T01:18:37.694791Z","iopub.status.idle":"2025-11-14T01:18:37.710960Z","shell.execute_reply.started":"2025-11-14T01:18:37.694770Z","shell.execute_reply":"2025-11-14T01:18:37.710265Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"model = SiameseUNetPP(n_channels=3, n_classes=1, base_ch=32)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\nprint(device)\ntrain_model(model,train_loader, val_loader, device, 5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T01:18:38.449314Z","iopub.execute_input":"2025-11-14T01:18:38.449598Z","iopub.status.idle":"2025-11-14T04:30:34.426494Z","shell.execute_reply.started":"2025-11-14T01:18:38.449576Z","shell.execute_reply":"2025-11-14T04:30:34.424826Z"}},"outputs":[{"name":"stdout","text":"cuda\n\nüåä Epoch [1/5]\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"üìä Train Loss: 1.4839 | Val Loss: 1.5541 | Val Dice: 0.1495\n‚úÖ Model improved ‚Äî saved to best_model.pth\n\nüåä Epoch [2/5]\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"üìä Train Loss: 1.4422 | Val Loss: 1.5339 | Val Dice: 0.1495\n‚úÖ Model improved ‚Äî saved to best_model.pth\n\nüåä Epoch [3/5]\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"üìä Train Loss: 1.4375 | Val Loss: 1.5381 | Val Dice: 0.1495\n‚ö†Ô∏è EarlyStopping counter: 1/3\n\nüåä Epoch [4/5]\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"üìä Train Loss: 1.4347 | Val Loss: 1.5407 | Val Dice: 0.1524\n‚ö†Ô∏è EarlyStopping counter: 2/3\n\nüåä Epoch [5/5]\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"üìä Train Loss: 1.4324 | Val Loss: 1.5389 | Val Dice: 0.1498\n‚ö†Ô∏è EarlyStopping counter: 3/3\nüõë Early stopping triggered.\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"SiameseUNetPP(\n  (enc_conv0): DoubleConv(\n    (conv): Sequential(\n      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (enc_conv1): DoubleConv(\n    (conv): Sequential(\n      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (enc_conv2): DoubleConv(\n    (conv): Sequential(\n      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (enc_conv3): DoubleConv(\n    (conv): Sequential(\n      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (enc_conv4): DoubleConv(\n    (conv): Sequential(\n      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (dconv3_0): DoubleConv(\n    (conv): Sequential(\n      (0): Conv2d(1536, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (dconv2_0): DoubleConv(\n    (conv): Sequential(\n      (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (dconv2_1): DoubleConv(\n    (conv): Sequential(\n      (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (dconv1_0): DoubleConv(\n    (conv): Sequential(\n      (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (dconv1_1): DoubleConv(\n    (conv): Sequential(\n      (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (dconv1_2): DoubleConv(\n    (conv): Sequential(\n      (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (dconv0_0): DoubleConv(\n    (conv): Sequential(\n      (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (dconv0_1): DoubleConv(\n    (conv): Sequential(\n      (0): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (dconv0_2): DoubleConv(\n    (conv): Sequential(\n      (0): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (dconv0_3): DoubleConv(\n    (conv): Sequential(\n      (0): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (out_convs): ModuleList(\n    (0-3): 4 x Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n  )\n)"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport rasterio\nfrom rasterio.windows import Window\nfrom skimage.measure import label, regionprops\nimport cv2\nimport pandas as pd\nfrom tqdm import tqdm\n\n\n# ---------------------------------------------------------\n# LOAD MODEL\n# ---------------------------------------------------------\ndef load_model(model_class, checkpoint, device):\n    model = model_class(n_channels=3, n_classes=1, base_ch=32)\n    model.load_state_dict(torch.load(checkpoint, map_location=device))\n    model.to(device)\n    model.eval()\n    return model\n\n\n# ---------------------------------------------------------\n# MODEL PREDICT (single tile)\n# ---------------------------------------------------------\ndef model_predict(model, pre_tile, post_tile, device):\n\n    pre_tile = pre_tile[:, :, :3]\n    post_tile = post_tile[:, :, :3]\n\n    pre = torch.tensor(pre_tile / 255., dtype=torch.float32).permute(2,0,1).unsqueeze(0).to(device)\n    post = torch.tensor(post_tile / 255., dtype=torch.float32).permute(2,0,1).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        out = model(pre, post)\n        pred = torch.sigmoid(out).squeeze().cpu().numpy()\n\n    del pre, post, out\n    torch.cuda.empty_cache()\n    return pred\n\n\n# ---------------------------------------------------------\n# EXTRACT REGION CENTROIDS + AREA\n# ---------------------------------------------------------\ndef extract_regions(mask, full_transform, pixel_size):\n\n    labelled = label(mask)\n    props = regionprops(labelled)\n\n    results = []\n\n    for region in props:\n        if region.area < 20:\n            continue\n\n        cy, cx = region.centroid\n        lon, lat = rasterio.transform.xy(full_transform, cy, cx)\n        area_m2 = region.area * (pixel_size ** 2)\n\n        results.append({\n            \"centroid_lon\": lon,\n            \"centroid_lat\": lat,\n            \"area_m2\": area_m2,\n        })\n\n    return results\n\n\n# ---------------------------------------------------------\n# SAFE IMAGE WRITER (JPG ONLY ‚Äî PREVENTS libpng ERRORS)\n# ---------------------------------------------------------\ndef safe_write_image(path, image):\n\n    # force JPG only\n    path = path.replace(\".png\", \".jpg\")\n\n    # ensure valid uint8\n    if image.dtype != np.uint8:\n        image = np.nan_to_num(image)\n        image = np.clip(image, 0, 255).astype(np.uint8)\n\n    # skip too small tiles\n    if image.shape[0] < 4 or image.shape[1] < 4:\n        return False\n\n    try:\n        cv2.imwrite(path, image, [cv2.IMWRITE_JPEG_QUALITY, 92])\n        return True\n    except:\n        return False\n\n\n\n# ---------------------------------------------------------\n# MAIN INFERENCE\n# ---------------------------------------------------------\ndef run_inference(\n    model,\n    pre_path,\n    post_path,\n    output_dir,\n    tile_size=512,\n    threshold=0.5,\n):\n\n    os.makedirs(output_dir, exist_ok=True)\n    cutout_dir = os.path.join(output_dir, \"cutouts\")\n    os.makedirs(cutout_dir, exist_ok=True)\n\n    device = next(model.parameters()).device\n    pre = rasterio.open(pre_path)\n    post = rasterio.open(post_path)\n\n    assert pre.shape == post.shape, \"ERROR: Pre/Post must be aligned!\"\n\n    H, W = pre.height, pre.width\n    base_transform = pre.transform\n    pixel_size = pre.res[0]\n\n    rows = (H + tile_size - 1) // tile_size\n    cols = (W + tile_size - 1) // tile_size\n\n    results = []\n    tile_id_counter = 0\n\n    for ty in tqdm(range(rows), desc=\"Rows\"):\n        for tx in range(cols):\n\n            x_off = tx * tile_size\n            y_off = ty * tile_size\n\n            w = min(tile_size, W - x_off)\n            h = min(tile_size, H - y_off)\n\n            window = Window(x_off, y_off, w, h)\n\n            # Read only required area\n            pre_arr = pre.read(window=window)\n            post_arr = post.read(window=window)\n\n            pre_tile = np.moveaxis(pre_arr, 0, 2).astype(np.uint8)\n            post_tile = np.moveaxis(post_arr, 0, 2).astype(np.uint8)\n\n            del pre_arr, post_arr\n\n            # Predict mask\n            pred = model_predict(model, pre_tile, post_tile, device)\n            mask = (pred > threshold).astype(np.uint8)\n\n            tile_transform = base_transform * rasterio.Affine.translation(x_off, y_off)\n            regions = extract_regions(mask, tile_transform, pixel_size)\n\n            # Skip saving tile images if NO regions detected\n            if len(regions) == 0:\n                continue\n\n            tile_id = f\"tile_{tile_id_counter:05d}\"\n            tile_id_counter += 1\n\n            pre_file = os.path.join(cutout_dir, f\"{tile_id}_pre.jpg\")\n            post_file = os.path.join(cutout_dir, f\"{tile_id}_post.jpg\")\n\n            # Make BGR for OpenCV\n            pre_img = cv2.cvtColor(pre_tile[:,:,:3], cv2.COLOR_RGB2BGR)\n            post_img = cv2.cvtColor(post_tile[:,:,:3], cv2.COLOR_RGB2BGR)\n\n            safe_write_image(pre_file, pre_img)\n            safe_write_image(post_file, post_img)\n\n            for region in regions:\n                results.append({\n                    \"tile_id\": tile_id,\n                    \"center_longitude\": region[\"centroid_lon\"],\n                    \"center_latitude\": region[\"centroid_lat\"],\n                    \"area_m2\": region[\"area_m2\"],\n                    \"area_lost_m2\": region[\"area_m2\"],\n                    \"pre_flood_land_image\": pre_file,\n                    \"post_flood_land_image\": post_file,\n                })\n\n            del pre_tile, post_tile, mask, pred, regions\n            torch.cuda.empty_cache()\n\n    df = pd.DataFrame(results)\n    df.to_csv(os.path.join(output_dir, \"affected.csv\"), index=False)\n\n    print(\"\\n‚úÖ Inference Complete!\")\n    print(\"Output directory:\", output_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T04:39:37.608016Z","iopub.execute_input":"2025-11-14T04:39:37.608696Z","iopub.status.idle":"2025-11-14T04:39:37.622819Z","shell.execute_reply.started":"2025-11-14T04:39:37.608672Z","shell.execute_reply":"2025-11-14T04:39:37.622107Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# from inference_flood_change import run_inference, load_model\n# from siamese_unetpp import SiameseUNetPP  # your model file\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# model = load_model(\n#     SiameseUNetPP,\n#     \"best_siamese_unetpp.pth\",\n#     device\n# )\n\nmodel = model.eval()\n\nrun_inference(\n    model=model,\n    pre_path=\"/kaggle/working/aligned/pre_aligned.tif\",\n    post_path=\"/kaggle/working/aligned/post_aligned.tif\",\n    output_dir=\"submissions/Dharay/\",\n    tile_size=512,\n    threshold=0.45\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}